<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>3D Model in Camera</title>
  <style>
    body, html {
      margin: 0;
      padding: 0;
      overflow: hidden;
    }
    #cameraFeed {
      position: absolute;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
      z-index: 0;
    }
    #threeCanvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      z-index: 1;
      touch-action: none;
    }
    #openBtn {
      position: absolute;
      top: 20px;
      left: 20px;
      z-index: 2;
      padding: 10px 20px;
      background: purple;
      color: white;
      border: none;
      border-radius: 8px;
      font-size: 16px;
    }
    #captureBtn {
      position: absolute;
      top: 20px;
      left: 150px;
      z-index: 2;
      padding: 10px 20px;
      background: #4CAF50;
      color: white;
      border: none;
      border-radius: 8px;
      font-size: 16px;
      display: none;
    }
    #resultText {
      position: absolute;
      top: 80px;
      left: 20px;
      z-index: 2;
      color: white;
      font-size: 24px;
      font-weight: bold;
      text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
      display: none;
    }
    #backgroundToggle {
      position: absolute;
      top: 20px;
      right: 20px;
      z-index: 2;
      padding: 10px 20px;
      background: #2196F3;
      color: white;
      border: none;
      border-radius: 8px;
      font-size: 16px;
      cursor: pointer;
    }
    #backgroundToggle:hover {
      background: #1976D2;
    }
    .background-options {
      position: absolute;
      top: 60px;
      right: 20px;
      z-index: 2;
      display: none;
      background: white;
      border-radius: 8px;
      padding: 10px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.2);
    }
    .background-option {
      padding: 8px 16px;
      cursor: pointer;
      border-radius: 4px;
      margin: 4px 0;
    }
    .background-option:hover {
      background: #f0f0f0;
    }
    .background-option.active {
      background: #e0e0e0;
    }
    .explanation-section {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      background: rgba(0, 0, 0, 0.8);
      color: white;
      padding: 20px;
      display: flex;
      gap: 20px;
      z-index: 2;
    }
    .explanation-left {
      flex: 1;
      padding: 20px;
      background: rgba(255, 255, 255, 0.1);
      border-radius: 8px;
    }
    .explanation-right {
      flex: 1;
      padding: 20px;
      background: rgba(255, 255, 255, 0.1);
      border-radius: 8px;
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
      gap: 15px;
      overflow-y: auto;
      max-height: 300px;
    }
    .video-item {
      cursor: pointer;
      transition: transform 0.2s;
    }
    .video-item:hover {
      transform: scale(1.05);
    }
    .video-thumbnail {
      width: 100%;
      border-radius: 4px;
    }
    .video-title {
      font-size: 14px;
      margin-top: 8px;
      color: white;
    }
    .play-button {
      background: #2196F3;
      color: white;
      border: none;
      padding: 8px 16px;
      border-radius: 4px;
      cursor: pointer;
      margin-top: 10px;
      display: flex;
      align-items: center;
      gap: 8px;
    }
    .play-button:hover {
      background: #1976D2;
    }
    .explanation-points {
      margin-top: 15px;
      line-height: 1.6;
    }
    .explanation-points li {
      margin-bottom: 8px;
    }
  </style>
  <!-- Load Three.js and its dependencies -->
  <script src="/static/js/three.min.js"></script>
  <script src="/static/js/GLTFLoader.js"></script>
  <!-- Add MediaPipe hand tracking -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands@0.4.1646424915/hands.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils@0.3/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils@0.3/drawing_utils.js"></script>
</head>
<body>
  <video id="cameraFeed" autoplay muted playsinline></video>
  <canvas id="threeCanvas"></canvas>
  <button id="openBtn">Open Camera</button>
  <button id="captureBtn">Take Photo</button>
  <button id="backgroundToggle">Change Background</button>
  <div class="background-options">
    <div class="background-option" data-bg="webcam">Webcam</div>
    <div class="background-option" data-bg="white">White</div>
    <div class="background-option" data-bg="black">Black</div>
  </div>
  <div id="resultText"></div>
  <div class="explanation-section">
    <div class="explanation-left">
      <h3>Heart Diagram Explanation</h3>
      <button class="play-button" onclick="speakExplanation()">
        <span>ðŸ”Š Play Explanation</span>
      </button>
      <ul class="explanation-points">
        <li>The heart is a muscular organ that pumps blood throughout the body</li>
        <li>It has four chambers: two atria and two ventricles</li>
        <li>The right side pumps blood to the lungs for oxygenation</li>
        <li>The left side pumps oxygenated blood to the body</li>
        <li>Valves prevent backflow of blood</li>
        <li>The heart beats about 100,000 times per day</li>
      </ul>
    </div>
    <div class="explanation-right" id="youtubeVideos">
      <!-- YouTube videos will be loaded here -->
    </div>
  </div>

  <script>
    // Wait for Three.js to load
    window.addEventListener('load', function() {
      console.log('Three.js loaded:', typeof THREE !== 'undefined');
      
      const video = document.getElementById('cameraFeed');
      const canvas = document.getElementById('threeCanvas');
      const openBtn = document.getElementById('openBtn');
      const captureBtn = document.getElementById('captureBtn');
      const resultText = document.getElementById('resultText');
      const backgroundToggle = document.getElementById('backgroundToggle');
      const backgroundOptions = document.querySelector('.background-options');
      const backgroundOptionElements = document.querySelectorAll('.background-option');
      
      const GEMINI_API_KEY = 'AIzaSyBHVWfY-js7KnjSQGuRe_Q0S2Tfu6W_cLI';

      let scene, camera, renderer, model;
      let hands;
      let lastPinchDistance = 0;
      let lastHandPosition = null;
      let targetZoom = 3;
      let targetRotation = { x: 0, y: 0 };
      const minZoom = 2.0;
      const maxZoom = 6.0;
      const zoomSpeed = 0.1;
      const zoomInterpolationFactor = 0.08;
      const rotationInterpolationFactor = 0.1;
      const rotationSensitivity = 3.0;
      let isHandTrackingActive = false;
      
      // Add label containers
      let labels = [];
      let labelContainer;
      
      // Add smoothing buffers
      const positionBuffer = [];
      const rotationBuffer = [];
      const bufferSize = 3;
      const movementThreshold = 0.005;

      // Add mouse control variables
      let isMouseDown = false;
      let lastMouseX = 0;
      let lastMouseY = 0;
      let mouseSensitivity = 0.01;
      let mouseZoomSpeed = 0.1;
      let isRightMouseDown = false;
      let lastRightMouseX = 0;
      let lastRightMouseY = 0;
      let mouseRotationSpeed = 0.005;
      let mousePanSpeed = 0.005;
      let isTrackpad = false;
      let lastWheelTime = 0;
      let wheelDelta = 0;

      let currentBackground = 'webcam';
      let isCameraActive = false;
      
      // Function to set background
      function setBackground(type) {
        currentBackground = type;
        
        // Update active state of options
        backgroundOptionElements.forEach(option => {
          option.classList.toggle('active', option.dataset.bg === type);
        });
        
        switch(type) {
          case 'webcam':
            video.style.display = 'block';
            canvas.style.backgroundColor = 'transparent';
            break;
          case 'white':
            video.style.display = 'none';
            canvas.style.backgroundColor = '#ffffff';
            break;
          case 'black':
            video.style.display = 'none';
            canvas.style.backgroundColor = '#000000';
            break;
        }
        
        // Update text color based on background
        if (type === 'white') {
          resultText.style.color = 'black';
          resultText.style.textShadow = '2px 2px 4px rgba(0, 0, 0, 0.2)';
        } else {
          resultText.style.color = 'white';
          resultText.style.textShadow = '2px 2px 4px rgba(0, 0, 0, 0.5)';
        }
        
        // Close options menu
        backgroundOptions.style.display = 'none';
      }

      // Toggle background options menu
      backgroundToggle.addEventListener('click', () => {
        backgroundOptions.style.display = 
          backgroundOptions.style.display === 'none' ? 'block' : 'none';
      });

      // Handle background option selection
      backgroundOptionElements.forEach(option => {
        option.addEventListener('click', () => {
          const bgType = option.dataset.bg;
          if (bgType === 'webcam' && !isCameraActive) {
            initCamera();
          }
          setBackground(bgType);
        });
      });

      // Function to capture model view
      function captureModelView(angle) {
        const tempCanvas = document.createElement('canvas');
        tempCanvas.width = 800;
        tempCanvas.height = 600;
        const tempRenderer = new THREE.WebGLRenderer({ 
          canvas: tempCanvas,
          alpha: true,
          antialias: true
        });
        
        // Create temporary camera
        const tempCamera = new THREE.PerspectiveCamera(75, 800/600, 0.1, 1000);
        tempCamera.position.set(0, 0, 5);
        
        // Rotate model for the view
        const originalRotation = model.rotation.clone();
        model.rotation.set(0, angle * Math.PI / 180, 0);
        
        // Render the view
        tempRenderer.render(scene, tempCamera);
        
        // Reset model rotation
        model.rotation.copy(originalRotation);
        
        return tempCanvas.toDataURL('image/jpeg');
      }

      // Function to create text label with simplified styling
      function createTextLabel(text, position, color = 0x000000) {
        const canvas = document.createElement('canvas');
        const context = canvas.getContext('2d');
        canvas.width = 300;
        canvas.height = 80;
        
        // Simple background
        context.fillStyle = 'rgba(0, 0, 0, 0.5)';
        context.fillRect(0, 0, canvas.width, canvas.height);
        
        // Simple text styling
        context.font = '20px Arial';
        context.fillStyle = '#ffffff';
        context.textAlign = 'left';
        context.fillText(text, 15, canvas.height/2 + 10);
        
        const texture = new THREE.CanvasTexture(canvas);
        const material = new THREE.SpriteMaterial({ 
          map: texture,
          transparent: true,
          opacity: 0.8
        });
        const sprite = new THREE.Sprite(material);
        
        sprite.position.copy(position);
        sprite.scale.set(1.2, 0.3, 1);
        sprite.userData = { 
          text: text,
          originalColor: color
        };
        
        return sprite;
      }

      // Function to create connecting line with simplified styling
      function createConnectingLine(start, end, color = 0x000000) {
        const material = new THREE.LineBasicMaterial({ 
          color: color,
          linewidth: 3,
          transparent: true,
          opacity: 0.8
        });
        const points = [];
        points.push(start);
        points.push(end);
        const geometry = new THREE.BufferGeometry().setFromPoints(points);
        const line = new THREE.Line(geometry, material);
        return line;
      }

      // Function to analyze model views
      async function analyzeModelViews() {
        try {
          // Capture front and side views
          const frontView = captureModelView(0);
          const sideView = captureModelView(90);
          
          // Prepare the request to Gemini API
          const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${GEMINI_API_KEY}`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              contents: [{
                parts: [
                  {
                    inline_data: {
                      mime_type: "image/jpeg",
                      data: frontView.split(',')[1]
                    }
                  },
                  {
                    inline_data: {
                      mime_type: "image/jpeg",
                      data: sideView.split(',')[1]
                    }
                  },
                  {
                    text: "These are front and side views of a 3D brain model. Analyze the brain structure and provide 5-6 labels describing different parts of the brain. For each label, specify its position in degrees (0-360) around the model. Return ONLY a valid JSON object in this exact format: {\"labels\": [{\"text\": \"label name\", \"angle\": \"angle in degrees\"}, ...]}. Do not include any other text or explanation."
                  }
                ]
              }]
            })
          });
          
          const data = await response.json();
          if (data.candidates && data.candidates[0].content.parts[0].text) {
            const result = data.candidates[0].content.parts[0].text;
            try {
              // Clean the response to ensure it's valid JSON
              const jsonStart = result.indexOf('{');
              const jsonEnd = result.lastIndexOf('}') + 1;
              const jsonStr = result.substring(jsonStart, jsonEnd);
              
              const labelsData = JSON.parse(jsonStr).labels;
              create3DLabels(labelsData);
            } catch (e) {
              console.error('Error parsing labels:', e);
              console.log('Raw response:', result);
            }
          }
        } catch (error) {
          console.error('Error analyzing model views:', error);
        }
      }

      // Initialize MediaPipe Hands
      function initHandTracking() {
        console.log('Initializing hand tracking...');
        hands = new Hands({
          locateFile: (file) => {
            return `https://cdn.jsdelivr.net/npm/@mediapipe/hands@0.4.1646424915/${file}`;
          }
        });
        
        hands.setOptions({
          maxNumHands: 1,  // Changed to 1 hand for better focus
          modelComplexity: 0,  // Changed to 0 for faster processing
          minDetectionConfidence: 0.7,  // Increased confidence threshold
          minTrackingConfidence: 0.7    // Increased confidence threshold
        });

        hands.onResults(onHandResults);
        console.log('Hand tracking initialized');
      }

      // Process hand tracking results with enhanced zoom
      function onHandResults(results) {
        if (!results.multiHandLandmarks) {
          if (isHandTrackingActive) {
            console.log('Hand lost, maintaining last position');
            isHandTrackingActive = false;
          }
          return;
        }

        if (!model) {
          console.log('Model not loaded yet');
          return;
        }

        isHandTrackingActive = true;
        const landmarks = results.multiHandLandmarks[0];
        
        if (landmarks) {
          const thumbTip = landmarks[4];
          const indexTip = landmarks[8];
          const palmBase = landmarks[0];
          
          // Enhanced pinch zoom with better sensitivity
          const pinchDistance = Math.sqrt(
            Math.pow(thumbTip.x - indexTip.x, 2) +
            Math.pow(thumbTip.y - indexTip.y, 2)
          );

          if (lastPinchDistance === 0) {
            lastPinchDistance = pinchDistance;
            lastHandPosition = { x: palmBase.x, y: palmBase.y };
            return;
          }

          // Calculate zoom factor with enhanced sensitivity
          const zoomFactor = lastPinchDistance / pinchDistance;
          if (Math.abs(1 - zoomFactor) > 0.03) {  // Reduced threshold for more responsive zoom
          const newTargetZoom = camera.position.z * zoomFactor;
          targetZoom = Math.max(minZoom, Math.min(maxZoom, newTargetZoom));
          }
          lastPinchDistance = pinchDistance;

          // Handle hand rotation with smoothing
          if (lastHandPosition) {
            // Calculate hand movement delta with increased sensitivity
            const deltaX = (palmBase.x - lastHandPosition.x) * rotationSensitivity;
            const deltaY = (palmBase.y - lastHandPosition.y) * rotationSensitivity;

            // Only update if movement is significant
            if (Math.abs(deltaX) > movementThreshold || Math.abs(deltaY) > movementThreshold) {
              // Add to rotation buffer
              rotationBuffer.push({ x: deltaX, y: deltaY });
              if (rotationBuffer.length > bufferSize) {
                rotationBuffer.shift();
              }

              // Calculate average rotation with reduced smoothing
              const avgRotation = rotationBuffer.reduce((acc, curr) => {
                return { x: acc.x + curr.x, y: acc.y + curr.y };
              }, { x: 0, y: 0 });
              
              const avgDeltaX = avgRotation.x / rotationBuffer.length;
              const avgDeltaY = avgRotation.y / rotationBuffer.length;

              // Update target rotation with increased movement
              targetRotation.y += avgDeltaX * 1.5;  // Increased rotation multiplier
              targetRotation.x += avgDeltaY * 1.5;  // Increased rotation multiplier

            // Clamp rotation to reasonable bounds
            targetRotation.x = Math.max(-Math.PI/2, Math.min(Math.PI/2, targetRotation.x));
            targetRotation.y = Math.max(-Math.PI, Math.min(Math.PI, targetRotation.y));
            }
          }

          lastHandPosition = { x: palmBase.x, y: palmBase.y };
        }
      }

      // Function to capture photo and send to Gemini
      async function captureAndAnalyze() {
        try {
          // Create a temporary canvas to capture the video frame
          const tempCanvas = document.createElement('canvas');
          tempCanvas.width = video.videoWidth;
          tempCanvas.height = video.videoHeight;
          const ctx = tempCanvas.getContext('2d');
          ctx.drawImage(video, 0, 0);
          
          // Convert canvas to base64
          const imageData = tempCanvas.toDataURL('image/jpeg');
          
          // Prepare the request to Gemini API
          const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${GEMINI_API_KEY}`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              contents: [{
                parts: [
                  {
                    inline_data: {
                      mime_type: "image/jpeg",
                      data: imageData.split(',')[1]
                    }
                  },
                  {
                    text: "Identify the main object in this image with a single word."
                  }
                ]
              }]
            })
          });
          
          const data = await response.json();
          if (data.candidates && data.candidates[0].content.parts[0].text) {
            const result = data.candidates[0].content.parts[0].text;
            resultText.textContent = result;
            resultText.style.display = 'block';
          }
        } catch (error) {
          console.error('Error analyzing image:', error);
          resultText.textContent = 'Error analyzing image';
          resultText.style.display = 'block';
        }
      }

      // Initialize the camera feed
      async function initCamera() {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ 
            video: { 
              width: 640,
              height: 480,
              facingMode: 'user'
            } 
          });
          video.srcObject = stream;
          isCameraActive = true;
          
          // Wait for video to be ready
          video.onloadedmetadata = () => {
            console.log('Video stream ready');
            // Show capture button
            captureBtn.style.display = 'block';
            // Initialize hand tracking after camera is ready
            initHandTracking();
            
            // Start sending frames to MediaPipe
            const camera = new Camera(video, {
              onFrame: async () => {
                try {
                  await hands.send({ image: video });
                } catch (error) {
                  console.error('Error processing hand tracking:', error);
                }
              },
              width: 640,
              height: 480
            });
            camera.start();
            console.log('Camera tracking started');
          };
        } catch (err) {
          console.error('Camera error:', err);
          // If camera fails, switch to black background
          setBackground('black');
        }
      }

      // Function to create arrow helper
      function createArrowHelper(start, end, color = 0xffffff) {
        const dir = new THREE.Vector3().subVectors(end, start).normalize();
        const length = start.distanceTo(end);
        const arrowHelper = new THREE.ArrowHelper(dir, start, length, color, length * 0.2, length * 0.1);
        return arrowHelper;
      }

      // Function to create 3D labels based on analysis
      function create3DLabels(labelsData) {
        if (!model) return;
        
        // Remove existing labels
        if (labelContainer) {
          scene.remove(labelContainer);
        }
        
        labelContainer = new THREE.Group();
        
        // Get model bounding box
        const box = new THREE.Box3().setFromObject(model);
        const size = box.getSize(new THREE.Vector3());
        const center = box.getCenter(new THREE.Vector3());
        
        // Define label positions around the model
        const radius = size.x * 1.2;  // Distance from center
        
        // Create labels and connecting lines based on the analyzed data
        labelsData.forEach(label => {
          // Calculate position around the model
          const angleRad = parseFloat(label.angle) * Math.PI / 180;
          const x = center.x + radius * Math.cos(angleRad);
          const z = center.z + radius * Math.sin(angleRad);
          const y = center.y;  // Default height, can be adjusted based on model
          
          const labelPosition = new THREE.Vector3(x, y, z);
          const targetPosition = new THREE.Vector3(
            center.x + size.x * 0.3 * Math.cos(angleRad),
            y,
            center.z + size.z * 0.3 * Math.sin(angleRad)
          );
          
          // Create text label
          const textLabel = createTextLabel(label.text, labelPosition);
          labelContainer.add(textLabel);
          
          // Create connecting line
          const line = createConnectingLine(targetPosition, labelPosition);
          labelContainer.add(line);
          
          // Add small circle at the model connection point
          const circleGeometry = new THREE.CircleGeometry(0.05, 32);
          const circleMaterial = new THREE.MeshBasicMaterial({ color: 0x000000 });
          const circle = new THREE.Mesh(circleGeometry, circleMaterial);
          circle.position.copy(targetPosition);
          circle.lookAt(camera.position);
          labelContainer.add(circle);
        });
        
        scene.add(labelContainer);
      }

      // Add mouse event handlers
      function initMouseControls() {
        // Detect if using trackpad
        canvas.addEventListener('wheel', (event) => {
          const now = Date.now();
          if (now - lastWheelTime < 50) {
            isTrackpad = true;
          }
          lastWheelTime = now;
          wheelDelta = event.deltaY;
        });

        // Left mouse button for rotation
        canvas.addEventListener('mousedown', (event) => {
          if (event.button === 0) { // Left mouse button
            isMouseDown = true;
            lastMouseX = event.clientX;
            lastMouseY = event.clientY;
            canvas.style.cursor = 'grabbing';
          } else if (event.button === 2) { // Right mouse button
            isRightMouseDown = true;
            lastRightMouseX = event.clientX;
            lastRightMouseY = event.clientY;
            canvas.style.cursor = 'move';
          }
        });

        // Mouse up event
        canvas.addEventListener('mouseup', (event) => {
          if (event.button === 0) {
            isMouseDown = false;
            canvas.style.cursor = 'grab';
          } else if (event.button === 2) {
            isRightMouseDown = false;
            canvas.style.cursor = 'default';
          }
        });

        // Mouse move event for rotation and panning
        canvas.addEventListener('mousemove', (event) => {
          if (!model) return;
          
          if (isMouseDown) {
            // Rotation with trackpad support
            const deltaX = event.clientX - lastMouseX;
            const deltaY = event.clientY - lastMouseY;
            
            // Adjust sensitivity based on whether using trackpad
            const rotationMultiplier = isTrackpad ? 0.5 : 1;
            
            targetRotation.y += deltaX * mouseRotationSpeed * rotationMultiplier;
            targetRotation.x += deltaY * mouseRotationSpeed * rotationMultiplier;
            
            // Clamp rotation
            targetRotation.x = Math.max(-Math.PI/2, Math.min(Math.PI/2, targetRotation.x));
            targetRotation.y = Math.max(-Math.PI, Math.min(Math.PI, targetRotation.y));
            
            lastMouseX = event.clientX;
            lastMouseY = event.clientY;
          } else if (isRightMouseDown) {
            // Panning with trackpad support
            const deltaX = event.clientX - lastRightMouseX;
            const deltaY = event.clientY - lastRightMouseY;
            
            // Adjust sensitivity based on whether using trackpad
            const panMultiplier = isTrackpad ? 0.5 : 1;
            
            // Calculate pan direction in world space
            const panX = deltaX * mousePanSpeed * panMultiplier;
            const panY = -deltaY * mousePanSpeed * panMultiplier; // Invert Y for natural movement
            
            // Apply pan to model position
            model.position.x += panX;
            model.position.y += panY;
            
            lastRightMouseX = event.clientX;
            lastRightMouseY = event.clientY;
          }
        });

        // Enhanced mouse wheel event for zoom with trackpad support
        canvas.addEventListener('wheel', (event) => {
          if (!model) return;
          
          // Calculate zoom direction and amount
          const zoomDirection = Math.sign(event.deltaY);
          
          // Adjust zoom speed based on whether using trackpad
          const zoomMultiplier = isTrackpad ? 0.5 : 1;
          const zoomAmount = zoomDirection * mouseZoomSpeed * zoomMultiplier;
          
          // Calculate new zoom target
          const newZoom = camera.position.z + zoomAmount;
          
          // Clamp zoom
          targetZoom = Math.max(minZoom, Math.min(maxZoom, newZoom));
          
          // Prevent default scroll behavior
          event.preventDefault();
        });

        // Prevent context menu on right click
        canvas.addEventListener('contextmenu', (event) => {
          event.preventDefault();
        });

        // Set initial cursor style
        canvas.style.cursor = 'grab';
      }

      // Function to fetch YouTube videos
      async function fetchYouTubeVideos() {
        const query = "heart diagram explanation";
        const apiKey = 'AIzaSyBiuZcFHVQO_mGOVuo_W6NBIOY9xnWSZ3Q';
        const maxResults = 6;
        
        try {
          // Use a different endpoint and add more parameters
          const response = await fetch(
            `https://www.googleapis.com/youtube/v3/search?` +
            `part=snippet&` +
            `q=${encodeURIComponent(query)}&` +
            `maxResults=${maxResults}&` +
            `type=video&` +
            `videoEmbeddable=true&` +
            `key=${apiKey}`
          );
          
          if (!response.ok) {
            const errorData = await response.json();
            console.error('YouTube API error details:', errorData);
            throw new Error(`YouTube API error: ${response.status} - ${errorData.error?.message || 'Unknown error'}`);
          }
          
          const data = await response.json();
          const videosContainer = document.getElementById('youtubeVideos');
          videosContainer.innerHTML = '';
          
          if (!data.items || !Array.isArray(data.items)) {
            throw new Error('Invalid YouTube API response format');
          }
          
          data.items.forEach(item => {
            if (!item.id || !item.id.videoId || !item.snippet) {
              console.warn('Skipping invalid video item:', item);
              return;
            }
            
            const videoId = item.id.videoId;
            const thumbnail = item.snippet.thumbnails?.medium?.url || '';
            const title = item.snippet.title || 'Untitled';
            
            const videoElement = document.createElement('div');
            videoElement.className = 'video-item';
            videoElement.innerHTML = `
              <img src="${thumbnail}" alt="${title}" class="video-thumbnail">
              <div class="video-title">${title}</div>
            `;
            
            videoElement.addEventListener('click', () => {
              window.open(`https://www.youtube.com/watch?v=${videoId}`, '_blank');
            });
            
            videosContainer.appendChild(videoElement);
          });
        } catch (error) {
          console.error('Error fetching YouTube videos:', error);
          const videosContainer = document.getElementById('youtubeVideos');
          videosContainer.innerHTML = `
            <div class="error-message">
              <p>Unable to load videos. Please try again later.</p>
              <p>Error: ${error.message}</p>
            </div>
          `;
        }
      }

      // Make speakExplanation globally accessible
      window.speakExplanation = function() {
        const explanation = document.querySelector('.explanation-points').textContent;
        const speech = new SpeechSynthesisUtterance(explanation);
        speech.rate = 1;
        speech.pitch = 1;
        window.speechSynthesis.speak(speech);
      };

      // Initialize Three.js scene and render
      function initThreeJS() {
        console.log('Initializing Three.js...');
        if (typeof THREE === 'undefined') {
          console.error('Three.js not loaded!');
          return;
        }

        scene = new THREE.Scene();
        camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
        camera.position.set(0, 0, 3);

        renderer = new THREE.WebGLRenderer({ 
          canvas: canvas, 
          alpha: true,
          antialias: true
        });
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.setPixelRatio(window.devicePixelRatio);
        renderer.setClearColor(0x000000, 0);

        // Add better lighting
        const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
        scene.add(ambientLight);

        const directionalLight = new THREE.DirectionalLight(0xffffff, 1);
        directionalLight.position.set(5, 5, 5);
        scene.add(directionalLight);

        const directionalLight2 = new THREE.DirectionalLight(0xffffff, 0.5);
        directionalLight2.position.set(-5, -5, -5);
        scene.add(directionalLight2);

        console.log('Three.js scene initialized');

        const loader = new THREE.GLTFLoader();
        console.log('Starting to load model...');
        
        loader.load('/static/model.glb', 
          function (gltf) {
            console.log('Model loaded successfully:', gltf);
            model = gltf.scene;
            
            // Center the model
            const box = new THREE.Box3().setFromObject(model);
            const center = box.getCenter(new THREE.Vector3());
            model.position.sub(center);

            // Scale the model appropriately
            const scale = 2.0;  // Increased scale
            model.scale.set(scale, scale, scale);
            
            // Position the model in front of the camera
            model.position.set(0, -1, -2);
            
            scene.add(model);
            console.log('Model added to scene');
            
            // Initialize mouse controls
            initMouseControls();
            
            // Analyze the model and create labels based on the actual model
            analyzeModelViews();
            
            // Fetch YouTube videos after model is loaded
            fetchYouTubeVideos().catch(error => {
              console.error('Failed to fetch YouTube videos:', error);
            });
            
            animate();
          }, 
          function (xhr) {
            console.log('Loading progress:', (xhr.loaded / xhr.total * 100) + '%');
          },
          function (error) {
            console.error('Error loading model:', error);
          }
        );
      }

      // Animation loop
      function animate() {
        requestAnimationFrame(animate);
        
        if (model) {
          // Smooth zoom interpolation
          const zoomDelta = targetZoom - camera.position.z;
          if (Math.abs(zoomDelta) > 0.001) {
            camera.position.z += zoomDelta * zoomInterpolationFactor;
          }
          
          // Smooth rotation interpolation
          const rotationDeltaX = targetRotation.x - model.rotation.x;
          const rotationDeltaY = targetRotation.y - model.rotation.y;
          
          if (Math.abs(rotationDeltaX) > 0.001 || Math.abs(rotationDeltaY) > 0.001) {
            model.rotation.x += rotationDeltaX * rotationInterpolationFactor;
            model.rotation.y += rotationDeltaY * rotationInterpolationFactor;
          }
        }
        
        // Update label container to always face camera
        if (labelContainer) {
          labelContainer.children.forEach(child => {
            if (child instanceof THREE.Sprite) {
              child.lookAt(camera.position);
            }
          });
        }
        
        renderer.render(scene, camera);
      }

      // Button to open the camera
      openBtn.addEventListener('click', () => {
        openBtn.style.display = 'none';
        initCamera();
        initThreeJS();
        setBackground('webcam');
      });

      // Add event listener for capture button
      captureBtn.addEventListener('click', captureAndAnalyze);

      // Handle window resize
      window.addEventListener('resize', () => {
        if (camera && renderer) {
          camera.aspect = window.innerWidth / window.innerHeight;
          camera.updateProjectionMatrix();
          renderer.setSize(window.innerWidth, window.innerHeight);
        }
      });

      // Initialize with black background
      setBackground('black');
    });
  </script>
</body>
</html>
